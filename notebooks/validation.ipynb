{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from hdfs3 import HDFileSystem\n",
    "from inference import run_search\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark add -l python -s groupAD -u http://iccluster044.iccluster.epfl.ch:8998 -k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the station names that appear the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = HDFileSystem(user=\"ebouille\")\n",
    "df_schedule_network = pd.DataFrame()\n",
    "for path in hdfs.glob(\"/user/anmaier/schedule_network.orc/hour=*/*.orc\"):\n",
    "    with hdfs.open(path) as f:\n",
    "        df_schedule_network = pd.concat((df_schedule_network, pd.read_orc(f)), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_stations = df_schedule_network[(df_schedule_network['src_stop_name'].value_counts() > 500)]\n",
    "top_n = 50\n",
    "top_stations = df_schedule_network.groupby('src_stop_name').count().sort_values('src_timestamp', ascending=False).iloc[:top_n, :].index.tolist()\n",
    "print(len(top_stations))\n",
    "df_schedule_network['src_stop_name'].value_counts().hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paths\n",
    "def get_random_time():\n",
    "        \"\"\"Generate a random time between 8am and 8pm as a timestamp a random day in 2022\"\"\"\n",
    "        # Generate random times until it satisfies these conditions:\n",
    "        # * it is between 8am and 8pm\n",
    "        # * it is not a sunday nor a saturday\n",
    "        while True:\n",
    "            # get random timestamps between 1st january 2022 and 1st january 2023\n",
    "            random_timestamp = np.random.randint(1640991600, 1672527600)\n",
    "            random_timestamp = pd.Timestamp.fromtimestamp(random_timestamp)\n",
    "            # get their date\n",
    "            random_date = random_timestamp.date()\n",
    "            # get a random time between 8am and 8pm\n",
    "            year = 2022\n",
    "            month = random_date.month\n",
    "            day = random_date.day\n",
    "            hour = int(8+12*np.random.rand())\n",
    "            minute = int(60*np.random.rand())\n",
    "            second = 0\n",
    "            random_time = pd.Timestamp(year=year, month=month, day=day, hour=hour, minute=minute, second=second)\n",
    "            # check if it is not a sunday nor a saturday\n",
    "            if random_time.weekday() not in [5, 6]:\n",
    "                break\n",
    "        return random_time\n",
    "\n",
    "responses = [] # list of responses given by the route planner for each query\n",
    "time_limits = pd.DataFrame()\n",
    "n_queries = 300#len(df_departures)\n",
    "path_counter = 0\n",
    "for i in tqdm(range(n_queries)):\n",
    "    departure = np.random.choice(top_stations)\n",
    "    while True:\n",
    "        arrival = np.random.choice(top_stations)\n",
    "        if arrival != departure:\n",
    "            break\n",
    "    random_time = get_random_time()\n",
    "    arrival_hour = random_time.hour\n",
    "    arrival_minute = random_time.minute\n",
    "    print(departure, arrival, random_time)\n",
    "    response = run_search(departure, arrival, arrival_hour, arrival_minute, 0.95, top_n=1, verbose=False, max_transfers=3, max_duration=60)\n",
    "    print(len(response))\n",
    "    if response != []:\n",
    "        responses.append(response)\n",
    "        for path in response:\n",
    "            path_counter += 1\n",
    "            time_limit = pd.DataFrame({'time_limit': [random_time],\n",
    "                                       'path_id': [path_counter]\n",
    "                                      })\n",
    "            time_limits = pd.concat((time_limits, time_limit), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store paths\n",
    "if False:\n",
    "    for i, response in enumerate(responses):\n",
    "        for j, path in enumerate(response):\n",
    "            file_path = \"/user/anmaier/validation/response=\" + str(i) + \"_path=\" + str(j) + \"/\"\n",
    "            with hdfs.open(file_path, 'wb') as f:\n",
    "                f.write(path.path_list.to_csv(index=False))\n",
    "# Load paths\n",
    "if True:\n",
    "    for file_path in hdfs.glob(\"/user/anmaier/validation/response=*_path=*/*.csv\"):\n",
    "        with hdfs.open(file_path) as f:\n",
    "            pd.read_csv(f).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_counter = 0\n",
    "transfers = pd.DataFrame(columns=[\"src_trip_id\", \"src_timestamp\", \"dst_trip_id\", \"dst_timestamp\", \"walking_duration\", \"path_id\"])\n",
    "last_trips = pd.DataFrame(columns=[\"trip_id\", \"dst_timestamp\", \"path_id\"])\n",
    "for i, response in enumerate(responses):\n",
    "    for j, path in enumerate(response):\n",
    "        path = path.path_list\n",
    "        path = path.reset_index(drop=True)\n",
    "        path_counter += 1\n",
    "        # Take last trip\n",
    "        # If no edge in the path, we stop there (should not happen with real data)\n",
    "        if len(path) == 0:\n",
    "            continue\n",
    "        last_trip = pd.DataFrame({'trip_id': [path.iloc[-2]['trip_id']],\n",
    "                          'dst_timestamp': [path.iloc[-2]['dst_timestamp']],\n",
    "                          'path_id': [path_counter]\n",
    "                         })\n",
    "        last_trips = pd.concat((last_trips, last_trip), ignore_index=True)\n",
    "        \n",
    "        # Take all situations where the user transfers by walking\n",
    "        # indexes of the walking edges\n",
    "        walking_edges_idxs = path[path.route_desc == \"walking\"].index\n",
    "        # If no walk, we stop there\n",
    "        if len(walking_edges_idxs) == 0:\n",
    "            continue\n",
    "        # Remove index 0 if it is a walk since we assume that \n",
    "        # a first walk has a probability 1 to be on time\n",
    "        if walking_edges_idxs[0] == 0:\n",
    "            walking_edges_idxs = walking_edges_idxs[1:]\n",
    "        if len(walking_edges_idxs) == 0:\n",
    "            continue\n",
    "        # Dataframe where each row is a change (happens when there is a walking edge)\n",
    "        path_transfers = pd.DataFrame()\n",
    "        path_transfers['src_trip_id'] = path.loc[walking_edges_idxs - 1, 'trip_id'].reset_index(drop=True)\n",
    "        path_transfers['src_timestamp'] = path.loc[walking_edges_idxs - 1, 'dst_timestamp'].reset_index(drop=True)\n",
    "        path_transfers['dst_trip_id'] = path.loc[walking_edges_idxs + 1, 'trip_id'].reset_index(drop=True)\n",
    "        path_transfers['dst_timestamp'] = path.loc[walking_edges_idxs + 1, 'src_timestamp'].reset_index(drop=True)\n",
    "        path_transfers['walking_duration'] = path.loc[walking_edges_idxs, 'walking_duration'].reset_index(drop=True)\n",
    "        # Add a column that is a unique identifier of the path\n",
    "        path_transfers['path_id'] = path_counter\n",
    "        # Append the transfers of the route to the transfers dataframe\n",
    "        transfers = pd.concat((transfers, path_transfers), ignore_index=True)\n",
    "        \n",
    "# We store the transfers and last_trips and time_limits csv files on hdfs\n",
    "transfers_csv = transfers.to_csv(index=False)\n",
    "last_trips_csv = last_trips.to_csv(index=False)\n",
    "time_limits_csv = time_limits.to_csv(index=False)\n",
    "with hdfs.open('/user/anmaier/validation/transfers.csv', 'wb') as f:\n",
    "    f.write(transfers_csv)\n",
    "with hdfs.open('/user/anmaier/validation/last_trips.csv', 'wb') as f:\n",
    "    f.write(last_trips_csv)\n",
    "with hdfs.open('/user/anmaier/validation/time_limits.csv', 'wb') as f:\n",
    "    f.write(time_limits_csv)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Read transfers\n",
    "df_transfers = (spark.read.csv(\"/user/anmaier/validation/transfers.csv\", header=True)\n",
    "                .withColumn(\"src_timestamp\", F.to_timestamp(\"src_timestamp\"))\n",
    "                .withColumn(\"dst_timestamp\", F.to_timestamp(\"dst_timestamp\"))\n",
    "                .withColumn(\"walking_duration\", F.col('walking_duration').cast(T.DoubleType()))\n",
    "                .withColumn('path_id', F.col('path_id').cast(T.IntegerType())))\n",
    "# Read last_trips\n",
    "df_last_trips = (spark.read.csv(\"/user/anmaier/validation/last_trips.csv\", header=True)\n",
    "                 .withColumn(\"dst_timestamp\", F.to_timestamp(\"dst_timestamp\"))\n",
    "                 .withColumn('path_id', F.col('path_id').cast(T.IntegerType())))\n",
    "# Read last_trips\n",
    "df_time_limits = (spark.read.csv(\"/user/anmaier/validation/time_limits.csv\", header=True)\n",
    "                 .withColumn(\"time_limit\", F.to_timestamp(\"time_limit\"))\n",
    "                 .withColumn('path_id', F.col('path_id').cast(T.IntegerType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Load test dataset\n",
    "is_stored = True\n",
    "if is_stored:\n",
    "    df_test = spark.read.orc(\"/user/anmaier/validation/df_test.orc\").cache()\n",
    "    df_test.count()\n",
    "else:\n",
    "    df_test = (spark.read.orc(\"/data/sbb/part_orc/istdaten/year=2022\")\n",
    "               .select(\"ANKUNFTSZEIT\", \"ABFAHRTSZEIT\", \"AN_PROGNOSE\", \"AB_PROGNOSE\", \"FAHRT_BEZEICHNER\", \"BETRIEBSTAG\")\n",
    "                # Rename columns\n",
    "               .withColumnRenamed(\"ANKUNFTSZEIT\", \"arrival_time\")\n",
    "               .withColumnRenamed(\"ABFAHRTSZEIT\", \"departure_time\")\n",
    "               .withColumnRenamed(\"AN_PROGNOSE\", \"true_arrival_time\")\n",
    "               .withColumnRenamed(\"AB_PROGNOSE\", \"true_departure_time\")\n",
    "               .withColumnRenamed(\"FAHRT_BEZEICHNER\", \"trip_id\")\n",
    "               .withColumnRenamed(\"BETRIEBSTAG\", \"date\")\n",
    "               # Format timestamps\n",
    "               .withColumn(\"arrival_time\", F.to_timestamp(\"arrival_time\", \"dd.MM.yyyy HH:mm\"))\n",
    "               .withColumn(\"departure_time\", F.to_timestamp(\"departure_time\", \"dd.MM.yyyy HH:mm\"))\n",
    "               .withColumn(\"true_arrival_time\", F.to_timestamp(\"true_arrival_time\", \"dd.MM.yyyy HH:mm:ss\"))\n",
    "               .withColumn(\"true_departure_time\", F.to_timestamp(\"true_departure_time\", \"dd.MM.yyyy HH:mm:ss\"))\n",
    "               .withColumn(\"date\", F.to_date('date', \"dd.MM.yyyy\"))\n",
    "               .cache())\n",
    "\n",
    "    df_test.write.orc(\"/user/anmaier/validation/df_test.orc\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Take date of generated time_limits\n",
    "dates = (df_time_limits\n",
    "         .withColumn('date', F.to_date('time_limit')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Modify transfers and last_trips with the new dates\n",
    "df_transfers = (df_transfers\n",
    "                 .withColumn('src_timestamp_str', F.date_format('src_timestamp', \"HH:mm:ss\"))\n",
    "                 .withColumn('dst_timestamp_str', F.date_format('dst_timestamp', \"HH:mm:ss\"))\n",
    "                 .join(dates, \"path_id\")\n",
    "                 .withColumn('src_timestamp', F.to_timestamp(F.concat(F.date_format('date', \"yyyy-MM-dd\"), F.lit(\" \"), F.col('src_timestamp_str'))))\n",
    "                 .withColumn('dst_timestamp', F.to_timestamp(F.concat(F.date_format('date', \"yyyy-MM-dd\"), F.lit(\" \"), F.col('dst_timestamp_str')))))\n",
    " # Weird error, using this tricks of saving the dataframe in a orc file and loading it again solves the issue\n",
    "df_transfers.write.orc(\"/user/anmaier/validation/temp_transfers.orc\", mode=\"overwrite\")\n",
    "df_last_trips = (df_last_trips\n",
    "                 .withColumn('dst_timestamp_str', F.date_format('dst_timestamp', \"HH:mm:ss\"))\n",
    "                 .join(dates, \"path_id\")\n",
    "                 .withColumn('dst_timestamp', F.to_timestamp(F.concat(F.date_format('date', \"yyyy-MM-dd\"), F.lit(\" \"), F.col('dst_timestamp_str')))))\n",
    "df_last_trips.write.orc(\"/user/anmaier/validation/temp_last_trips.orc\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "temp = (spark.read.orc(\"/user/anmaier/validation/temp_transfers.orc\")\n",
    "        # Format data\n",
    "        .withColumn(\"src_timestamp\", F.to_timestamp(\"src_timestamp\"))\n",
    "        .withColumn(\"dst_timestamp\", F.to_timestamp(\"dst_timestamp\")))\n",
    "(temp\n",
    " .join(df_test.select('arrival_time', 'trip_id', 'true_arrival_time'),\n",
    "       (temp.src_timestamp == df_test.arrival_time)\n",
    "       & (temp.src_trip_id == df_test.trip_id))\n",
    " .write.orc(\"/user/anmaier/validation/temp_transfers2.orc\", mode=\"overwrite\"))\n",
    "# Again we use the same tricks\n",
    "temp = (spark.read.orc(\"/user/anmaier/validation/temp_transfers2.orc\")\n",
    "        .withColumn(\"src_timestamp\", F.to_timestamp(\"src_timestamp\"))\n",
    "        .withColumn(\"dst_timestamp\", F.to_timestamp(\"dst_timestamp\")))\n",
    "transfers_test = (temp\n",
    "                .join(df_test.select('departure_time', 'trip_id', 'true_departure_time'),\n",
    "                      (temp.dst_timestamp == df_test.departure_time)\n",
    "                      & (temp.dst_trip_id == df_test.trip_id))\n",
    "                  .select('true_departure_time', 'true_arrival_time', 'departure_time', 'arrival_time', 'walking_duration', 'path_id')\n",
    "                  .na.drop())\n",
    "\n",
    "transfers_yes = (transfers_test\n",
    "                # True if we missed our transfer\n",
    "                 .withColumn('test', \n",
    "                             (F.unix_timestamp('true_departure_time') - F.unix_timestamp('true_arrival_time')).cast(T.DoubleType())\n",
    "                             < F.col('walking_duration') * 60.)\n",
    "                 .withColumn('test', F.col('test').cast(T.IntegerType()))\n",
    "                 .withColumn('path_id', F.col('path_id').cast(T.IntegerType()))\n",
    "                 .groupBy('path_id')\n",
    "                 .sum('test')\n",
    "                 .withColumn('result_transfers', (F.col('sum(test)') == 0).cast(T.IntegerType())))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "temp = (spark.read.orc(\"/user/anmaier/validation/temp_last_trips.orc\")\n",
    "        # Format data\n",
    "        .withColumn(\"dst_timestamp\", F.to_timestamp(\"dst_timestamp\")))\n",
    "test_temp = df_test.select('arrival_time', 'trip_id', 'true_arrival_time')\n",
    "last_trips_test = (temp\n",
    "                    .join(test_temp,\n",
    "                          (temp.dst_timestamp == test_temp.arrival_time)\n",
    "                          & (temp.trip_id == test_temp.trip_id))\n",
    "                   .select('true_arrival_time', 'arrival_time', 'time_limit', 'path_id')\n",
    "                   .na.drop())\n",
    "                   \n",
    "last_trips_yes = (last_trips_test\n",
    "                  # True if we did not arrive in time\n",
    "                  .withColumn('test',\n",
    "                              F.unix_timestamp('time_limit') < F.unix_timestamp('true_arrival_time'))\n",
    "                  .withColumn('test', F.col('test').cast(T.IntegerType()))\n",
    "                  .withColumn('path_id', F.col('path_id').cast(T.IntegerType()))\n",
    "                  .groupBy('path_id')\n",
    "                  .sum('test')\n",
    "                  .withColumn('result_last_trips', (F.col('sum(test)') == 0).cast(T.IntegerType())))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "joined_yes = last_trips_yes.join(transfers_yes, 'path_id', 'left').fillna(-1)\n",
    "results = (joined_yes\n",
    "           .filter(~(F.col('result_transfers') == F.lit(0)) | (F.col('result_last_trips') == F.lit(0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "n_path = joined_yes.count()\n",
    "\n",
    "n_path_yesyesssss = results.count()\n",
    "print(n_path_yesyesssss, n_path)\n",
    "if n_path != 0:\n",
    "    pourcentage = n_path_yesyesssss / n_path\n",
    "    print(pourcentage)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 queries -> 15 path test, 15 path yes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
