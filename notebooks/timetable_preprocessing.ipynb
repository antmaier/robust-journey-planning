{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tl;dr: This notebook constructs the schedule network. Load it from `/user/anmaier/schedule_network.orc`, it is partitioned by `hour`. \n",
    "You can access the stops near Zurich HB from `/user/anmaier/df_stops_near.orc`.\n",
    "\n",
    "This notebook constructs the schedule network for a date (`year-month-day`) user-defined below.\n",
    "We assume that the timetable is the same for the whole year. It also compute the relevant stops near Zurich HB (i.e. the ones that are part of a trip that are at least partially within a 15 km radius from Zurich HB).\n",
    "\n",
    "## Schedule network\n",
    "\n",
    "The schedule network is constructed based on `istdaten` dataset for the specified date, using in particular the scheduled arrival and departure times. It is partitioned by hour, and saved in `/user/anmaier/schedule_network.orc`. It is a spark dataframe where each row represents an edge with the following schema:\n",
    "* `src_timestamp (TimestampType)`: timestamp of the source\n",
    "* `src_stop_name (StringType)`: name of the source station\n",
    "* `dst_timestamp (TimestampType)`: timestamp of the destination\n",
    "* `dst_stop_name (StringType)`: name of the destination station\n",
    "* `route_desc (StringType)`: type of the mean of transport (*Zug/Bus/Tram/Schiff/...* for a *trip* edge, *walking* for a walking edge and *waiting* for a waiting edge)\n",
    "* `trip_id (StringType)`: id of the trip (non-empty for *trip* and *waiting* edges, empty for *walking* edges)\n",
    "* `distance (DoubleType)`: distance of the edge in km\n",
    "* `duration (IntegerType)`: duration of the edge in min\n",
    "* `probability (DoubleType)`: probability of the edge (relevant only for *walking* edges). !!! All the probabilites are set 1 for now, they are computed afterwards in `probabilities.py` !!!\n",
    "* `walking_duration (DoubleType)`: time it would take to walk from source to destination with a delay of 2 minutes and a speed of 50m/min\n",
    "\n",
    "__Nomenclature__: \n",
    "Data in  `istdaten` dataset represent a transport arriving at a station (arrival) and leaving the station (departure). For the schedule network we use instead source and destination, which does not necessarily correspond to a departure and an arrival of a transport but really to the source and destination of an edge. There are three types of edges:\n",
    "* A *waiting* edge is when we wait in a station without leaving the mean of transport\n",
    "* A *trip* edge is the travel between two consecutive stations without changing the mean of transport\n",
    "* A *walking* edge is any situation where we leave the mean of transport and walk to another one (either another platform of the same station or another station)\n",
    "\n",
    "## Stops near Zurich HB\n",
    "\n",
    "We also compute the stops near Zurich HB (i.e. the ones that are part of a trip that are at least partially within a 15 km radius from Zurich HB). They are saved in `/user/anmaier/df_stops_near.orc`. It is a spark dataframe with the following schema:\n",
    "* `stop_name (StringType)`: name of the stop\n",
    "* `stop_lat (DoubleType)`: latitude of the stop\n",
    "* `stop_lon (DoubleType)`: longitude of the stop\n",
    "* `in_radius (BooleanType)`: whether the stop is within a 15 km radius from Zurich HB\n",
    "\n",
    "__Nomenclature__:\n",
    "A stop is *near* if it is part of a trip that has at least one stop within a 15 km radius from Zurich HB. A stop is additionally *in radius* if it is within a 15 km radius from Zurich HB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting up the Spark runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark add -l python -s groupAD -u http://iccluster044.iccluster.epfl.ch:8998 -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "df_schedule_network = spark.read.orc(\"/user/anmaier/schedule_network.orc\").cache()\n",
    "df_stops_near = spark.read.orc(\"/user/anmaier/df_stops_near.orc\").cache()\n",
    "df_timetable_near = spark.read.orc(\"/user/anmaier/df_timetable_near.orc\").cache()\n",
    "df_edges = spark.read.orc(\"/user/anmaier/df_edges.orc\").cache()\n",
    "df_waiting_edges = spark.read.orc(\"/user/anmaier/df_waiting_edges.orc\").cache()\n",
    "df_trip_edges = spark.read.orc(\"/user/anmaier/df_trip_edges.orc\").cache()\n",
    "df_walking_edges = spark.read.orc(\"/user/anmaier/df_walking_edges.orc\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "(df_schedule_network\n",
    "     .withColumn('hour', F.hour('dst_timestamp')) # use dst_timestamp for hour instead\n",
    "     .write.partitionBy('hour').orc(\"/user/anmaier/schedule_network.orc\", mode=\"overwrite\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# !!!! Think before modifying it\n",
    "# If True, will overwrite the dataframes on hdfs\n",
    "store = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import graphframes as gf\n",
    "\n",
    "from math import sin, cos, sqrt, atan2, radians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date of reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Specify the date of reference\n",
    "# Not all days are available in the timetables dataset\n",
    "year = \"2022\"\n",
    "month = \"4\" # 1 to 12, do not write a '0' prefix, i.e. write 4 not 04\n",
    "day = \"27\" # Idem, write 4 not 04 for example\n",
    "# 4th anniversary of the Panmunjom Declaration \\o/\n",
    "\n",
    "# Get string date in yyyy-MM-dd format, ensuring that month and day are 2-digits numbers\n",
    "_month, _day = month, day\n",
    "if len(month) == 1:\n",
    "    _month = \"0\" + month\n",
    "if len(day) == 1:\n",
    "    _day = \"0\" + day\n",
    "date = year + \"-\" + _month + \"-\" + _day\n",
    "date_col = F.to_date(F.lit(date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "@F.udf(returnType=T.DoubleType())\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the distance between two points on the Earth's surface using the WGS84 ellipsoid.\n",
    "    \n",
    "    Args:\n",
    "        lat1 (float): Latitude of the first point in degrees.\n",
    "        lon1 (float): Longitude of the first point in degrees.\n",
    "        lat2 (float): Latitude of the second point in degrees.\n",
    "        lon2 (float): Longitude of the second point in degrees.\n",
    "        \n",
    "    Returns:\n",
    "        float: The distance between the two points in kilometers.\n",
    "    \"\"\"\n",
    "    # Approximate radius of earth in km\n",
    "    R = 6378.0\n",
    "\n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(lon1)\n",
    "    lat2 = radians(lat2)\n",
    "    lon2 = radians(lon2)\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    \n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "# !! It is NOT an UDF, it's on purpose\n",
    "def duration(src_timestamp, dst_timestamp) -> T.IntegerType():\n",
    "    \"\"\"Compute the time diff between destination and source in minutes\"\"\"\n",
    "    return ((F.unix_timestamp(dst_timestamp) - F.unix_timestamp(src_timestamp))/60).cast(T.IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "df_istdaten = spark.read.orc(\"/data/sbb/part_orc/istdaten/year=\" + year + \"/month=\" + month)\n",
    "df_stops = spark.read.orc(\"/data/sbb/part_orc/timetables/stops/year=\" + year + \"/month=\" + month + \"/day=\" + day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "def print_df(df: DataFrame, df_name: str = \"\", n: int = 3, truncate: bool = True) -> None:\n",
    "    \"\"\"Prints the schema and the first n rows of a DataFrame.\"\"\"\n",
    "    if df_name:\n",
    "        print(df_name + \":\")\n",
    "    print([a[1] for a in df.dtypes])\n",
    "    df.show(n=n, truncate=truncate)\n",
    "    return None\n",
    "\n",
    "# Print datasets\n",
    "print_df(df_istdaten, \"istdaten\")\n",
    "print_df(df_stops, \"stops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove non-standard rows, rename and select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# window function to add a stop_sequence column that is a stop counter for each trip_id\n",
    "# i.e. the first stop has stop_sequence=0, the 2nd stop stop_sequence=1, etc \n",
    "# up to the last stop of the trip\n",
    "w_stop_sequence = Window.partitionBy('trip_id').orderBy('arrival_time')\n",
    "\n",
    "_df_istdaten = (df_istdaten\n",
    "               # Remove non-standard rows\n",
    "               .filter(F.col('ZUSATZFAHRT_TF') == F.lit(\"false\"))\n",
    "               .filter(F.col('DURCHFAHRT_TF') == F.lit(\"false\"))\n",
    "               .filter((F.col('ab_prognose_status') != '') & (F.col('an_prognose_status') != ''))\n",
    "               # Rename columns\n",
    "               .withColumnRenamed('BETRIEBSTAG', 'date')\n",
    "               .withColumnRenamed('HALTESTELLEN_NAME', 'stop_name')\n",
    "               .withColumnRenamed('FAHRT_BEZEICHNER', 'trip_id')\n",
    "               .withColumnRenamed('ANKUNFTSZEIT', 'arrival_time')\n",
    "               .withColumnRenamed('ABFAHRTSZEIT', 'departure_time')\n",
    "               .withColumnRenamed('PRODUKT_ID', 'route_desc')\n",
    "               # Format date and timestamps\n",
    "               .withColumn('date', F.to_date('date', 'dd.MM.yyyy'))\n",
    "               .withColumn('arrival_time', F.to_timestamp('arrival_time', 'dd.MM.yyyy HH:mm'))\n",
    "               .withColumn('departure_time', F.to_timestamp('departure_time', 'dd.MM.yyyy HH:mm'))\n",
    "               # Filter the date\n",
    "               .filter(F.col('date') == F.to_date(F.lit(date)))\n",
    "               # Remove rows where both arrival and departure time are null or nan\n",
    "               .filter(~((F.col('arrival_time').isNull()) & (F.col('departure_time').isNull())))\n",
    "               # Add a stop_sequence column\n",
    "               .withColumn('stop_sequence', F.rank().over(w_stop_sequence))\n",
    "               # Select only the columns we need\n",
    "               .select('date', 'stop_name', 'trip_id', 'arrival_time', 'departure_time', 'route_desc', 'stop_sequence'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep only stops that are near Zurich HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Add a column to indicate if a stop is within the `distance_max` radius centered on Zurich HB\n",
    "distance_max = 15.0 # maximum distance from Zurich HB in km\n",
    "zurich_HB_lat = 47.378177\n",
    "zurich_HB_lon = 8.540211\n",
    "_df_stops = (df_stops\n",
    "            .withColumn('in_radius', \n",
    "                        distance(F.lit(zurich_HB_lat), F.lit(zurich_HB_lon), \n",
    "                                 F.col('stop_lat'), F.col('stop_lon')) \n",
    "                        <= distance_max))\n",
    "\n",
    "# Get the id of all trips that has at least one stop within a `distance_max` km radius centered on Zurich HB\n",
    "df_trip_ids_near = (_df_istdaten\n",
    "                    .join(_df_stops.filter(F.col('in_radius') == True), \n",
    "                          on='stop_name', how='leftsemi')\n",
    "                    .select('trip_id')\n",
    "                    .distinct())\n",
    "\n",
    "# Filter istdaten keeping only trips that enter the radius\n",
    "df_timetable_near = (_df_istdaten\n",
    "                    .join(df_trip_ids_near, 'trip_id', 'leftsemi')\n",
    "                    .cache())\n",
    "# and get their coresponding stops\n",
    "df_stops_near = (_df_stops\n",
    "                 .join(df_timetable_near, 'stop_name', 'leftsemi')\n",
    "                 .dropDuplicates(['stop_name'])\n",
    "                 .select('stop_name', 'stop_lat', 'stop_lon', 'in_radius')\n",
    "                 .cache())\n",
    "# Finally we filter the stops in the timetable for which we don't have the latitude and longitude\n",
    "df_timetable_near = df_timetable_near.join(df_stops_near, 'stop_name', 'left_semi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "print(df_timetable_near.show(n=3), df_timetable_near.count())\n",
    "print(df_stops_near.show(n=3), df_stops_near.count())\n",
    "if store:\n",
    "    df_timetable_near.write.orc(\"/user/anmaier/df_timetable_near.orc\", mode=\"overwrite\")\n",
    "    df_stops_near.write.orc(\"/user/anmaier/df_stops_near.orc\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "edge_schema = T.StructType([\n",
    "    T.StructField('src_timestamp', T.TimestampType(), False),\n",
    "    T.StructField('src_stop_name', T.StringType(), False),\n",
    "    T.StructField('dst_timestamp', T.TimestampType(), False),\n",
    "    T.StructField('dst_stop_name', T.StringType(), False),\n",
    "    T.StructField('route_desc', T.StringType(), False),\n",
    "    T.StructField('trip_id', T.StringType(), False),\n",
    "    T.StructField('dst_trip_id', T.StringType(), False), # extra col for dst_trip_id\n",
    "    T.StructField('distance', T.DoubleType(), False), # in km\n",
    "    T.StructField('duration', T.IntegerType(), False), # in min\n",
    "    T.StructField('probability', T.DoubleType(), False)\n",
    "])\n",
    "# Create the dataframe that will contain all the edges\n",
    "df_edges = spark.createDataFrame([], edge_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Waiting edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Waiting edges, i.e. waiting in a transport at a stop without leaving the transport\n",
    "df_waiting_edges = (df_timetable_near\n",
    "                    # Remove start of journeys\n",
    "                    .filter(~F.col('arrival_time').isNull())\n",
    "                    .filter(~F.col('departure_time').isNull())\n",
    "                    # Rename times to match schema\n",
    "                    .withColumnRenamed('arrival_time', 'src_timestamp')\n",
    "                    .withColumnRenamed('departure_time', 'dst_timestamp')\n",
    "                    # Add source and destination stop name\n",
    "                    .withColumn('src_stop_name', F.col('stop_name'))\n",
    "                    .withColumn('dst_stop_name', F.col('stop_name'))\n",
    "                    # Add route_desc\n",
    "                    .withColumn('route_desc', F.lit('waiting'))\n",
    "                    # Add 0km distance\n",
    "                    .withColumn('distance', F.lit(0.0))\n",
    "                    # Add duration\n",
    "                    .withColumn('duration', duration('src_timestamp', 'dst_timestamp'))\n",
    "                    # Add proba\n",
    "                    .withColumn('probability', F.lit(1.))\n",
    "                    # Filter and order columns to match the schema\n",
    "                    .select('src_timestamp', \n",
    "                            'src_stop_name',\n",
    "                            'dst_timestamp', \n",
    "                            'dst_stop_name',\n",
    "                            'route_desc', \n",
    "                            'trip_id', \n",
    "                            'distance',\n",
    "                            'duration',\n",
    "                            'probability')\n",
    "                    .cache())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "print(df_waiting_edges.show(n=3), df_waiting_edges.count())\n",
    "if store:\n",
    "    df_waiting_edges.write.orc(\"/user/anmaier/df_waiting_edges.orc\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trip edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "df_timetable_near = spark.read.orc(\"/user/anmaier/df_timetable_near.orc\")\n",
    "df_stops_near = spark.read.orc(\"/user/anmaier/df_stops_near.orc\")\n",
    "# A trip edge connects a departure from a location to an arrival to another location\n",
    "# Two vertices are connected if they belong to the same trip \n",
    "# and if the destination stop has stop_sequence incremented by 1 compared to the source stop.\n",
    "\n",
    "# Format sources and destinations with infos needed for constructing trip edges\n",
    "df_sources = (df_timetable_near\n",
    "                 # Exclude end of journeys (null departure time)\n",
    "                 .filter(~F.col('departure_time').isNull())\n",
    "                 # Take latitude and longitude of the stop\n",
    "                 .join(df_stops_near, on='stop_name')\n",
    "                 .withColumnRenamed('stop_lat', 'src_lat')\n",
    "                 .withColumnRenamed('stop_lon', 'src_lon')\n",
    "                 # To distinguish between sources and destinations columns\n",
    "                 .withColumnRenamed('departure_time', 'src_timestamp')\n",
    "                 .withColumnRenamed('stop_name', 'src_stop_name')\n",
    "                 .distinct()\n",
    "                 .select('*'))\n",
    "df_destinations = (df_timetable_near\n",
    "                 # Exclude start of journeys (null arrival time)\n",
    "                 .filter(~F.col('arrival_time').isNull())\n",
    "                 # Take latitude and longitude of the stop\n",
    "                 .join(df_stops_near, on='stop_name')\n",
    "                 .withColumnRenamed('stop_lat', 'dst_lat')\n",
    "                 .withColumnRenamed('stop_lon', 'dst_lon')\n",
    "                 # To distinguish between sources and destinations columns\n",
    "                 .withColumnRenamed('arrival_time', 'dst_timestamp')\n",
    "                 .withColumnRenamed('stop_name', 'dst_stop_name')\n",
    "                 .distinct()\n",
    "                 .select('dst_timestamp', 'dst_stop_name', 'stop_sequence', \n",
    "                         'trip_id', 'dst_lat', 'dst_lon'))\n",
    "\n",
    "# Then we construct the trip edges\n",
    "df_trip_edges = (df_sources\n",
    "                 # Increment stop_sequence of departures to match the next arrival stop_sequence\n",
    "                 .withColumn('stop_sequence', F.col('stop_sequence') + 1)\n",
    "                 # Join departures to arrival that have the same trip_id and stop_sequence \n",
    "                 .join(df_destinations, on=['trip_id', 'stop_sequence'])\n",
    "                 # Add ditance, duration and proba\n",
    "                 .withColumn('distance', distance('src_lat', 'src_lon', 'dst_lat', 'dst_lon'))\n",
    "                 .withColumn('duration', duration('src_timestamp', 'dst_timestamp'))\n",
    "                 .withColumn('probability', F.lit(1.))\n",
    "                 # Filter and order columns to match the schema\n",
    "                 .select('src_timestamp', \n",
    "                         'src_stop_name', \n",
    "                         'dst_timestamp', \n",
    "                         'dst_stop_name', \n",
    "                         'route_desc', \n",
    "                         'trip_id', \n",
    "                         'distance', \n",
    "                         'duration',\n",
    "                         'probability')\n",
    "                 .distinct()\n",
    "                 .cache())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "print(df_trip_edges.show(n=3), df_trip_edges.count())\n",
    "if store:\n",
    "    df_trip_edges.write.orc(\"/user/anmaier/df_trip_edges.orc\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Walking edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# A walking edge connect an arrival to all departures that are reachable by walking.\n",
    "# It is reachable if these conditions holds:\n",
    "# 1. The distance between the arrival and departure is at most 500m\n",
    "# 2. The departure time is at least 2 minutes (leaving the transport) + 1min/50m (walking) away from the arrival time\n",
    "# 3. The departure time is at most `max_duration` minutes away from the arrival time\n",
    "# 4. For each stop name an arrival is connected to, we only keep the `max_n_destinations_per_stop` ones that are closest in time\n",
    "max_distance = 0.5 # in km\n",
    "leave_duration = 2 # in min\n",
    "velocity = 0.05 # in km/min\n",
    "max_duration = 30 # Max duration we allow to walk in minutes\n",
    "max_n_destinations_per_stop = 3 # Maximum number of destinations per stop we connect an arrival to\n",
    "w_max_n_destinations_per_stop = (Window\n",
    "                                 .partitionBy('src_stop_name', 'dst_stop_name')\n",
    "                                 .orderBy(duration('src_timestamp', 'dst_timestamp')))\n",
    "\n",
    "# For computational reasons, we will split the day in bins of `bin_duration` seconds \n",
    "# and do the cross join between 1 sources bin and all the necessary destinations bins \n",
    "# so that the last timestamp in sources bin can walk up to `max_duration` seconds.\n",
    "# For example if max_duration = 30 minutes and bin_duration = 10 minutes\n",
    "# then we will cross join one sources bin with 3+1 destinations bins\n",
    "bin_duration = 10 # Duration of the bins in minutes\n",
    "n_bins = int(24*60 / bin_duration) # Number of bins per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Format sources and destinations with infos needed for constructing walking edges\n",
    "start_day = F.to_timestamp(F.lit(date)) # Beginning of the day\n",
    "df_sources = (df_timetable_near\n",
    "              # Exclude start of journeys (null arrival time)\n",
    "              .filter(~F.col('arrival_time').isNull())\n",
    "              # Take latitude and longitude of the stop\n",
    "              .join(df_stops_near, on='stop_name')\n",
    "              .withColumnRenamed('stop_lat', 'src_lat')\n",
    "              .withColumnRenamed('stop_lon', 'src_lon')\n",
    "              # To distinguish between sources and destinations columns\n",
    "              .withColumnRenamed('arrival_time', 'src_timestamp')\n",
    "              .withColumnRenamed('stop_name', 'src_stop_name')\n",
    "              # We indicate to which nth bin they belong\n",
    "              .withColumn('src_bin', F.floor(duration(date_col, 'src_timestamp') / bin_duration))\n",
    "              .select('src_timestamp', 'src_stop_name', 'src_lat', 'src_lon', 'src_bin', 'trip_id') # keep the trip_id of the source trip as trip_id\n",
    "              .distinct()\n",
    "              .cache())\n",
    "if store:\n",
    "    df_sources.write.orc(\"/user/anmaier/df_sources.orc\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "df_destinations = (df_timetable_near\n",
    "                  # Exclude end of journeys (null departure time)\n",
    "                  .filter(~F.col('departure_time').isNull())\n",
    "                  # Take latitude and longitude of the stop\n",
    "                  .join(df_stops_near, on='stop_name')\n",
    "                  .withColumnRenamed('stop_lat', 'dst_lat')\n",
    "                  .withColumnRenamed('stop_lon', 'dst_lon')\n",
    "                  # To distinguish between sources and destinations columns\n",
    "                  .withColumnRenamed('departure_time', 'dst_timestamp')\n",
    "                  .withColumnRenamed('stop_name', 'dst_stop_name')\n",
    "                  .withColumnRenamed('trip_id', 'dst_trip_id')\n",
    "                  # We indicate to which nth bin they belong\n",
    "                  .withColumn('dst_bin', F.floor(duration(date_col, 'dst_timestamp') / bin_duration))\n",
    "                  .select('dst_timestamp', 'dst_stop_name', 'dst_lat', 'dst_lon', 'dst_bin', 'dst_trip_id') # keep the trip_id of the dest trip as dst_trip_id\n",
    "                  .distinct()\n",
    "                  .cache())\n",
    "if store:\n",
    "    df_destinations.write.orc(\"/user/anmaier/df_destinations.orc\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the walking edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Store each bin individually\n",
    "walking_edges_bins = {}\n",
    "for i in range(0, n_bins - int(max_duration/bin_duration)):\n",
    "    walking_edges_bins[i] = (df_sources\n",
    "                            .filter(F.col('src_bin') == i)\n",
    "                            .crossJoin(df_destinations\n",
    "                                       .filter((F.col('dst_bin') >= i) & (F.col('dst_bin') <= i + int(max_duration/bin_duration))))\n",
    "                            # Filter the combinations that are reachable by walking\n",
    "                            .withColumn('distance', distance('src_lat', 'src_lon', 'dst_lat', 'dst_lon'))\n",
    "                            .filter(F.col('distance') <= max_distance)\n",
    "                            .withColumn('duration', duration('src_timestamp', 'dst_timestamp'))\n",
    "                            .filter(F.col('duration') >= leave_duration + F.col('distance') / velocity)\n",
    "                            .filter(F.col('duration') <= max_duration)\n",
    "                            # Add route_desc, trip_id\n",
    "                            .withColumn('route_desc', F.lit(\"walking\"))\n",
    "                            #.withColumn('trip_id', F.lit(\"\")) # no longer want this to be empty\n",
    "                            .withColumn('probability', F.lit(1.))\n",
    "                            # Filter and order columns to match the schema\n",
    "                            .select('src_timestamp', \n",
    "                                    'src_stop_name', \n",
    "                                    'dst_timestamp', \n",
    "                                    'dst_stop_name', \n",
    "                                    'route_desc', \n",
    "                                    'trip_id',\n",
    "                                    'dst_trip_id', # keep dst_trip_id as well\n",
    "                                    'distance', \n",
    "                                    'duration',\n",
    "                                    'probability'))\n",
    "    if store:\n",
    "        walking_edges_bins[i].write.orc(\"/user/anmaier/walking_edges/bin=\" + str(i), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Union into slices of 10 bins\n",
    "walking_edges_slices = {}\n",
    "for i in range(0, n_bins - int(max_duration/bin_duration), 10):\n",
    "    walking_edges_slices[i] = spark.createDataFrame([], edge_schema)\n",
    "    for j in range(i, min(i + 10, n_bins - int(max_duration/bin_duration))):\n",
    "        walking_edges_slices[i] = walking_edges_slices[i].union(spark.read.orc(\"/user/anmaier/walking_edges/bin=\" + str(j)))\n",
    "    if store:\n",
    "        walking_edges_slices[i].write.orc(\"/user/anmaier/walking_edges/slice=\" + str(i), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Union of all the slices\n",
    "df_walking_edges = spark.createDataFrame([], edge_schema)\n",
    "for i in range(0, n_bins - int(max_duration/bin_duration), 10):\n",
    "    df_walking_edges = df_walking_edges.union(spark.read.orc(\"/user/anmaier/walking_edges/slice=\" + str(i)))\n",
    "# Remove walking edges when too many are connected to the same stop_name\n",
    "df_walking_edges = (df_walking_edges\n",
    "                    .withColumn('rank', F.rank().over(w_max_n_destinations_per_stop))\n",
    "                    .filter(F.col('rank') <= max_n_destinations_per_stop)\n",
    "                    .drop('rank'))\n",
    "if store:\n",
    "    df_walking_edges.write.orc(\"/user/anmaier/df_walking_edges.orc\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union all edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "df_waiting_edges = spark.read.orc(\"/user/anmaier/df_waiting_edges.orc\")\n",
    "df_trip_edges = spark.read.orc(\"/user/anmaier/df_trip_edges.orc\")\n",
    "\n",
    "# create columns for dst_trip_id in the waiting and trip edges (duplicate of trip_id so its easier to join later)\n",
    "df_waiting_edges = df_waiting_edges.withColumn('dst_trip_id', F.col(\"trip_id\"))\n",
    "df_trip_edges = df_trip_edges.withColumn('dst_trip_id', F.col(\"trip_id\"))\n",
    "\n",
    "df_walking_edges = spark.read.orc(\"/user/anmaier/df_walking_edges.orc\")\n",
    "df_edges = (df_waiting_edges\n",
    "            .union(df_trip_edges)\n",
    "            .union(df_walking_edges)\n",
    "            .distinct())\n",
    "# Add a column with the time it would take to walk from src to dst\n",
    "df_edges = df_edges.withColumn('walking_duration', 2. + F.col('distance') / 0.05)\n",
    "if store:\n",
    "    df_edges.write.orc(\"/user/anmaier/df_edges.orc\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Partition by hour\n",
    "if store:\n",
    "    (df_edges\n",
    "     .withColumn('hour', F.hour('dst_timestamp')) # use dst_timestamp for hour instead\n",
    "     .write.partitionBy('hour').orc(\"/user/anmaier/schedule_network.orc\", mode=\"overwrite\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
